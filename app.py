import uvicorn
from fastapi import FastAPI, Request
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import numpy as np
import joblib
import torch
from transformers import AutoTokenizer, AutoModel
import contextlib
import re
import unicodedata
from underthesea import word_tokenize
import os

# --- C·∫§U H√åNH ---
STOPWORDS_FILE = 'vietnamese-stopwords.txt'
MODEL_PATH = 'vinai/phobert-base' # Load Online t·ª´ HuggingFace

# --- 1. ƒê·ªäNH NGHƒ®A D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO ---
class NewsInput(BaseModel):
    source: str
    title: str
    content: str

# --- 2. BI·∫æN TO√ÄN C·ª§C (ƒê·ªÉ l∆∞u model ƒë√£ load) ---
ml_models = {}
STOPWORDS = set()

# --- 3. C√ÅC H√ÄM H·ªñ TR·ª¢ (HELPER FUNCTIONS) ---

def load_stopwords_global():
    """ƒê·ªçc file stopwords v√† l∆∞u v√†o b·ªô nh·ªõ"""
    global STOPWORDS
    try:
        if os.path.exists(STOPWORDS_FILE):
            with open(STOPWORDS_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word:
                        STOPWORDS.add(word)
                        # Th√™m c·∫£ bi·∫øn th·ªÉ c√≥ g·∫°ch d∆∞·ªõi (cho underthesea)
                        STOPWORDS.add(word.replace(' ', '_'))
            print(f"‚úÖ ƒê√£ t·∫£i {len(STOPWORDS)} t·ª´ d·ª´ng.")
        else:
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file '{STOPWORDS_FILE}'. B·ªè qua b∆∞·ªõc l·ªçc stopword.")
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc stopwords: {e}")

def text_preprocess(text):
    """
    Quy tr√¨nh l√†m s·∫°ch d·ªØ li·ªáu (Ph·∫£i kh·ªõp 100% v·ªõi l√∫c Train)
    1. Unicode -> 2. Regex -> 3. Lower -> 4. Tokenize -> 5. Stopwords
    """
    if not isinstance(text, str) or text is None:
        return ""

    # B1: Chu·∫©n h√≥a Unicode
    text = unicodedata.normalize('NFC', text)

    # B2: Regex l√†m s·∫°ch (B·ªè HTML, URL, Email, K√Ω t·ª± ƒë·∫∑c bi·ªát, S·ªë)
    text = re.sub(r'<[^>]*>', ' ', text)
    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'\S*@\S*\s?', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)

    # B3: Chuy·ªÉn th∆∞·ªùng & B·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()

    # B4: T√°ch t·ª´ (Tokenize) b·∫±ng Underthesea
    try:
        text = word_tokenize(text, format='text')
    except:
        pass # N·∫øu l·ªói th√¨ gi·ªØ nguy√™n text

    # B5: Lo·∫°i b·ªè Stopwords
    if text and STOPWORDS:
        words = text.split()
        clean_words = [w for w in words if w not in STOPWORDS]
        text = " ".join(clean_words)

    return text

def get_embeddings(text_list, max_len=256):
    """Chuy·ªÉn ƒë·ªïi text th√†nh vector b·∫±ng PhoBERT"""
    tokenizer = ml_models['tokenizer']
    bert = ml_models['bert']
    device = ml_models['device']
    
    inputs = tokenizer(
        text_list, 
        return_tensors='pt', 
        padding=True, 
        truncation=True, 
        max_length=max_len
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = bert(**inputs)
    
    # L·∫•y vector CLS (token ƒë·∫ßu ti√™n)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy()

# --- 4. LIFESPAN (QU·∫¢N L√ù KH·ªûI ƒê·ªòNG SERVER) ---
@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    print("\n‚è≥ ƒêang kh·ªüi ƒë·ªông Server & T·∫£i Models...")
    
    # 1. Load Stopwords
    load_stopwords_global()

    # 2. Load Scikit-learn Models
    try:
        ml_models['le'] = joblib.load('models/label_encoder.pkl')
        ml_models['mlp'] = joblib.load('models/news_classifier_mlp.pkl')
        print("‚úÖ ƒê√£ t·∫£i MLP Model & Label Encoder.")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i file .pkl: {e}")
        print("üí° G·ª£i √Ω: Ki·ªÉm tra xem file ƒë√£ n·∫±m trong th∆∞ m·ª•c 'models/' ch∆∞a?")

    # 3. Load PhoBERT (Online)
    try:
        print(f"‚è≥ ƒêang t·∫£i PhoBERT t·ª´ {MODEL_PATH} (c√≥ th·ªÉ m·∫•t 30s)...")
        ml_models['tokenizer'] = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)
        ml_models['bert'] = AutoModel.from_pretrained(MODEL_PATH)
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        ml_models['bert'].to(device)
        ml_models['device'] = device
        print(f"‚úÖ ƒê√£ t·∫£i xong PhoBERT (Device: {device})")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i PhoBERT: {e}")

    print("üöÄ Server ƒë√£ s·∫µn s√†ng!\n")
    yield
    # D·ªçn d·∫πp khi t·∫Øt
    ml_models.clear()

# --- 5. KH·ªûI T·∫†O APP & ROUTES ---
app = FastAPI(lifespan=lifespan)
templates = Jinja2Templates(directory="templates")

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Tr·∫£ v·ªÅ giao di·ªán HTML"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/predict")
async def predict(data: NewsInput):
    """API D·ª± ƒëo√°n ch√≠nh"""
    try:
        # 1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Pre-processing)
        # Quan tr·ªçng: Ph·∫£i l√†m s·∫°ch Title v√† Content tr∆∞·ªõc khi ƒë∆∞a v√†o PhoBERT
        clean_title = text_preprocess(data.title)
        clean_content = text_preprocess(data.content)

        # 2. Feature Engineering
        # Token count: T√≠nh tr√™n n·ªôi dung g·ªëc (theo logic manual test c≈©)
        token_count = len(data.content.split())
        
        # Embeddings: T√≠nh tr√™n n·ªôi dung ƒê√É L√ÄM S·∫†CH
        title_emb = get_embeddings([clean_title], max_len=64)
        content_emb = get_embeddings([clean_content], max_len=256)
        
        # Source Hash: Logic c≈©
        source_feat = np.array([[hash(data.source) % 1000]])
        token_feat = np.array([[token_count]])
        
        # 3. G·ªôp features (Stacking)
        full_features = np.hstack([title_emb, content_emb, source_feat, token_feat])
        
        # 4. D·ª± ƒëo√°n (Inference)
        mlp = ml_models['mlp']
        le = ml_models['le']
        
        pred_idx = mlp.predict(full_features)[0]
        label = le.inverse_transform([pred_idx])[0]
        
        # T√≠nh x√°c su·∫•t tin c·∫≠y (Confidence score)
        pred_proba = mlp.predict_proba(full_features).max()
        
        return {
            "status": "success",
            "prediction": label,
            "confidence": f"{pred_proba*100:.2f}%"
        }

    except Exception as e:
        return {"status": "error", "message": str(e)}

# --- 6. CH·∫†Y SERVER ---
if __name__ == '__main__':
    # L∆∞u √Ω: Khi ch·∫°y file n√†y tr·ª±c ti·∫øp, kh√¥ng d√πng reload ƒë·ªÉ tr√°nh l·ªói Windows
    uvicorn.run(app, host="127.0.0.1", port=8000)